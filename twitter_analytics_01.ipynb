{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis of Twitter Data\n",
    "\n",
    "## Play with Twitter Streaming API\n",
    "API stands for Application Programming Interface. It is a tool that makes the interaction with computer programs and web services easy. Many web services provides APIs to developers to interact with their services and to access data in programmatic way. For this programming experiment, we will use Twitter Streaming API to download tweets related to the 2 keywords: \"**big data**\", and \"**data analytic**\".\n",
    "### Step 1: Getting Twitter API keys\n",
    "In order to access Twitter Streaming API, we need to get 4 pieces of information from Twitter: *API key*, *API secret*, *Access token* and *Access token secret*. Follow the steps below to get all these 4 elements:\n",
    "* Create a twitter account if you do not already have one.\n",
    "* Go to https://apps.twitter.com/ and log in with your twitter credentials.\n",
    "* Click \"Create New App\"\n",
    "* Fill out the form, agree to the terms, and click \"Create your Twitter application\"\n",
    "* In the next page, click on \"API keys\" tab, and copy your \"API key\" and \"API secret\".\n",
    "* Scroll down and click \"Create my access token\", and copy your \"Access token\" and \"Access token secret\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Variables that contains the user credentials to access Twitter API\n",
    "access_token = \"2259908755-6E8kBo4s4jn22nCss2Up6Pi5ohqaU5nhtkqMyG7\"\n",
    "access_token_secret = \"YopfcIU3JfXDcravZB8ImZQk6ZLFvPAjbct0ac9MFCoBB\"\n",
    "consumer_key = \"2cexrYWchxAZqF4SD5eNm2TlI\"\n",
    "consumer_secret = \"y0Ji0lruxZ0uc490n5gIIatQ1N7RJLjaVpRxF3nytqCCMCv7cC\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Connecting to Twitter Streaming API and downloading data\n",
    "We will be using a Python library called **Tweepy** to connect to Twitter Streaming API and downloading the data.\n",
    "\n",
    "If you don't have Tweepy installed in your machine, go to this link [https://github.com/tweepy/tweepy], and follow the installation instructions.\n",
    "\n",
    "You can also run '*pip install tweepy*' in your anaconda installed directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Nov 29 09:56:35 +0000 2016 #bigdata needs to be #qualitydata @BartsMSBlog https://t.co/qDVmz0aaWQ\n",
      "Tue Nov 29 09:56:39 +0000 2016 RT @bynse: .@gonzamartind de @bynse participa en inauguración #AgridataSummit 2016 exponiendo las razones de este evento sobre… \n",
      "Tue Nov 29 09:56:45 +0000 2016 ¿Por qué Machine Learning Big Data juega un papel impor.. #Tecnología https://t.co/sbh9IeN2sl #Paraunmejorfuturo https://t.co/az01PBf54P\n",
      "Tue Nov 29 09:56:53 +0000 2016 RT @huerta_digital: ¿Será el #BigData el cambio disruptivo que necesita el sector agrario para ser más sostenible? #AgridataSummit\n",
      "Tue Nov 29 09:56:55 +0000 2016 Locaweb Corp lança primeiro serviço de Big Data no Brasil https://t.co/xELJIhbb8b\n",
      "Tue Nov 29 09:57:00 +0000 2016 https://t.co/DtJPnNlzy0 #DataScience #Stata https://t.co/mXYwNIugC9\n",
      "Tue Nov 29 09:57:01 +0000 2016 Internet of things Industrial big data  Industrial big data Questions You Should Ask Professionals https://t.co/uPmyWwGf9h\n",
      "Tue Nov 29 09:57:00 +0000 2016 RT @gpivetta75: #IoT y #BigData : oportunidad de aprender más sobre cómo se comportan los clientes #Insurtech @mlourdesrr @icemd https://t.…\n",
      "Tue Nov 29 09:57:04 +0000 2016 RT @AgefiFrance: Objets connectés, #bigdata, l’assurance doit réinventer ses modèles, avec Nicolas Montetagaud… \n",
      "Tue Nov 29 09:57:01 +0000 2016 @PleasureEthics it's taken me over 30 years to trust my instincts, so I love this learning #opendata #smartcities #bigdata #ai\n",
      "Tue Nov 29 09:57:06 +0000 2016 RT @DrPas84Ele: How to move forward #research into the #future. #CHANGE is coming up thks to #bioinformatic #bigdata &amp;… \n",
      "Tue Nov 29 09:57:06 +0000 2016 RT @myagonism: @ingdave ( @eBay EU #analytics) \" #bigdata won where the #esperanto failed ;) .\" #OBDA16\n",
      "Tue Nov 29 09:57:07 +0000 2016 RT @marcoatbossi: #IoT, #BigData and VR are the top trends in the current #health #Digitaltransformation. https://t.co/ocRYKiwdVY https://t…\n",
      "Tue Nov 29 09:57:10 +0000 2016 Data Science Stata  Stata Resources Only Leaders Should Know About https://t.co/Av5WdYTSs9\n",
      "Tue Nov 29 09:57:12 +0000 2016 RT @wiomax: #Healthcare and the #ArtificialIntelligence revolution https://t.co/6Lz2bZiya4 #AI #HealthTech #DeepLearning… \n",
      "Tue Nov 29 09:57:12 +0000 2016 RT @agrnegocios: En @agrnegocios asistimos al primer #AgridataSummit organizado por @bynse &amp; @La_COAG, el evento de referencia del… \n",
      "Tue Nov 29 09:57:13 +0000 2016 RT @BigDataIoT_FR: Le #CDO, un accélérateur et un créateur de valeur pour l'entreprise ! #BigData via @Viuzfr https://t.co/y0JGShBDcg\n",
      "Tue Nov 29 09:57:15 +0000 2016 RT @UnitecoPro: .@ihmedrano en el @InstitutoRoche: el #BigData es el análisis predictivo de millones de datos, al servicio de la… \n",
      "Tue Nov 29 09:57:18 +0000 2016 Can #bigdata improve #infectiousdiseases surveillance? #complexnetworks #Complexsystems #publichealth… https://t.co/wki9M3Yocc\n",
      "Tue Nov 29 09:57:27 +0000 2016 RT @innovationbawue: #Daten, Keywords, Analysen - die #Ebner Verlagsgruppe aus #Ulm will den #Journalismus umkrempeln https://t.co/pvH1eIPC…\n",
      "Tue Nov 29 09:57:28 +0000 2016 RT @InstitutoRoche: \"La clave está en complementar el análisis de los #datos estructurados y no estructurados\" @ihmedrano #IRValorSNS… \n",
      "Tue Nov 29 09:57:31 +0000 2016 RT @ImDataScientist: 34 Must-Read Blogs for the Latest on #Content #Marketing https://t.co/7km8jix2OJ #BigData #DataScience https://t.co/k0…\n",
      "Tue Nov 29 09:57:31 +0000 2016 Miguel Blanco de @La_COAG: \"El verdadero progreso es el que pone la #tecnología al alcance de todos\"… https://t.co/kUH0uB9OhI\n",
      "Tue Nov 29 09:57:34 +0000 2016 RT @KirkDBorne: 7 Top Data Visualization Books for Inquisitive Minds: https://t.co/tbK6FVz6tN #abdsc #bigdata #DataViz #DataScience… \n",
      "Tue Nov 29 09:57:34 +0000 2016 Six Big Topics at @CloudExpo | #BigData #IoT #DevOps #DigitalTransformation https://t.co/NILCLC03eD\n",
      "Tue Nov 29 09:57:36 +0000 2016 @axellelemaire l'ergonomie/appropriation point faible des innovations françaises? Vs. leadership tricolore dans les… https://t.co/jsHjkysKMY\n",
      "Tue Nov 29 09:57:38 +0000 2016 Learn How @Informatica Customers Unlock Value with #BigData https://t.co/LMG0qfihkF https://t.co/oQyE0uX5ei\n",
      "Tue Nov 29 09:57:42 +0000 2016 10 Emerging Technologies That Will Drive The Next Economy ! #BigData #IoT\n",
      "https://t.co/GzyDJFUB1M https://t.co/g6asntKTSJ\n",
      "Tue Nov 29 09:57:49 +0000 2016 1,2 Mrd. globale  #mobile #IoT Verbindungen bis 2021! #BigData\n",
      "https://t.co/gYCz5zvvlw https://t.co/8tZ0lrnVl1\n",
      "Tue Nov 29 09:57:52 +0000 2016 Mejor uso de los recursos naturales para producir alimentos, con las nuevas tecnologías\" Isabel García Tejerina #AgridataSummit #bigdata\n",
      "Tue Nov 29 09:57:56 +0000 2016 Your Location &amp; #Bigdata Journey with @AvareaOy @quuppa &amp; @ionsign at #nexterday via @ComptelCorp &amp; @finproFi https://t.co/eWfPETHEIY\n",
      "Tue Nov 29 09:57:57 +0000 2016 RT @KirkDBorne: What are the components of a #SmartCity? https://t.co/tqFFXflEEw #SmartCities #BigData #IoT #DataScience #AI… \n",
      "Tue Nov 29 09:58:00 +0000 2016 Speedy #DigitalTransformation | @CloudExpo @Apcela @ipfconline1\n",
      "https://t.co/Ps1eqIfbNK #IoT #AI #BigData #socbiz… https://t.co/4pYXWueKU2\n",
      "Tue Nov 29 09:58:00 +0000 2016 Register now! Join @googlecloud , @Infectiousmedia &amp; #Looker  for a #data #webinar on Dec. 6th  #bigdata #bigquery https://t.co/VCLLeY4rGd\n",
      "Tue Nov 29 09:58:01 +0000 2016 RT @ipfconline1: To Fear or Not to Fear #Robots, That's the Question \n",
      "by @fmarotob via @Datafloq https://t.co/O0QOQ5OZXU   [#BigData… \n",
      "Tue Nov 29 09:58:02 +0000 2016 RT @CODE_n: What is #BigData? It’s based on these four criteria. Find out more: https://t.co/PRd8rUkLaN #CODE_n #DataScience… \n",
      "Tue Nov 29 09:58:03 +0000 2016 12 #BigData Predictions You Should Not Miss. https://t.co/DllY3W3fGd https://t.co/Nfua0E1slO\n",
      "Tue Nov 29 09:58:04 +0000 2016 RT @david_green_uk: Delighted to have penned the foreword to this excellent book by @AnalyticsinHR #PeopleAnalytics #HR #futureofwork… \n",
      "Tue Nov 29 09:58:03 +0000 2016 12 #BigData Predictions You Should Not Miss. https://t.co/DllY3W3fGd https://t.co/Nfua0E1slO\n",
      "Tue Nov 29 09:58:04 +0000 2016 RT @david_green_uk: Delighted to have penned the foreword to this excellent book by @AnalyticsinHR #PeopleAnalytics #HR #futureofwork… \n",
      "Tue Nov 29 09:58:06 +0000 2016 Next Generation #IoT Tools: #Beacons | @ThingsExpo #M2M #API #ML #BIgData https://t.co/rE10Uzxmwn #Technology #TechNews\n",
      "Tue Nov 29 09:58:12 +0000 2016 RT @IoTRecruiting: The IOT / Big Data and Data Scientists - https://t.co/ycRAnP07zH #Industry40 #IndustrialInternet #IIoT #IOT #IoE #Intern…\n",
      "Tue Nov 29 09:58:15 +0000 2016 #BigData #analytics #datascience #data https://t.co/oAbmnlobJW\n",
      "Tue Nov 29 09:58:18 +0000 2016 RT @PleasureEthics: Love this, the experience of accepting impartial advice, after it was taken #opendata #smartcities #bigdata #ai… \n",
      "Tue Nov 29 09:58:21 +0000 2016 RT @PleasureEthics: @PleasureEthics yet they've written the equivalent of a few years learning for free? #opendata #smartcities #bigdata #a…\n",
      "Tue Nov 29 09:58:21 +0000 2016 RT @PleasureEthics: @PleasureEthics yet they've written the equivalent of a few years learning for free? #opendata #smartcities #bigdata #a…\n",
      "Tue Nov 29 09:58:22 +0000 2016 Big Challenges of #BigData | @CloudExpo @Gemalto #Security #AI #ML #DL https://t.co/1omavc7vOH #Technology #TechNews\n",
      "Tue Nov 29 09:58:23 +0000 2016 RT @PleasureEthics: @PleasureEthics it's taken me over 30 years to trust my instincts, so I love this learning #opendata #smartcities #bigd…\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-f378043e5501>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[1;31m#This line filter Twitter Streams to capture data by the keywords: 'python', 'javascript', 'ruby'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m     \u001b[0mtwitter_stream\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrack\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'big data'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'data analytic'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'data science'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'#bigdata'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'#datascience'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'#dataanalytic'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m     \u001b[1;31m#twitter_stream.filter(track=['*'], languages=['th'])\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[1;31m#twitter_stream.filter(track=['*'])\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\knotsupavit\\Anaconda2\\lib\\site-packages\\tweepy\\streaming.pyc\u001b[0m in \u001b[0;36mfilter\u001b[0;34m(self, follow, track, async, locations, stall_warnings, languages, encoding, filter_level)\u001b[0m\n\u001b[1;32m    443\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparams\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'delimited'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'length'\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    444\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhost\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'stream.twitter.com'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 445\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_start\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0masync\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    446\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m     def sitestream(self, follow, stall_warnings=False,\n",
      "\u001b[0;32mC:\\Users\\knotsupavit\\Anaconda2\\lib\\site-packages\\tweepy\\streaming.pyc\u001b[0m in \u001b[0;36m_start\u001b[0;34m(self, async)\u001b[0m\n\u001b[1;32m    359\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_thread\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    360\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 361\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_run\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    362\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    363\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mon_closed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\knotsupavit\\Anaconda2\\lib\\site-packages\\tweepy\\streaming.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    261\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msnooze_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msnooze_time_step\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlistener\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_connect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 263\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_read_loop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    264\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mTimeout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mssl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSSLError\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m                 \u001b[1;31m# This is still necessary, as a SSLError can actually be\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\knotsupavit\\Anaconda2\\lib\\site-packages\\tweepy\\streaming.pyc\u001b[0m in \u001b[0;36m_read_loop\u001b[0;34m(self, resp)\u001b[0m\n\u001b[1;32m    311\u001b[0m             \u001b[0mlength\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    312\u001b[0m             \u001b[1;32mwhile\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mresp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraw\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclosed\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 313\u001b[0;31m                 \u001b[0mline\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbuf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_line\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    314\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mline\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlistener\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeep_alive\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# keep-alive new lines are expected\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\knotsupavit\\Anaconda2\\lib\\site-packages\\tweepy\\streaming.pyc\u001b[0m in \u001b[0;36mread_line\u001b[0;34m(self, sep)\u001b[0m\n\u001b[1;32m    177\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m                 \u001b[0mstart\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_buffer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_buffer\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stream\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_chunk_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_pop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlength\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\knotsupavit\\Anaconda2\\lib\\site-packages\\requests\\packages\\urllib3\\response.pyc\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, amt, decode_content, cache_content)\u001b[0m\n\u001b[1;32m    312\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m                 \u001b[0mcache_content\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 314\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    315\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mamt\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# Platform-specific: Buggy versions of Python.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m                     \u001b[1;31m# Close the connection when no data is returned\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\knotsupavit\\Anaconda2\\lib\\httplib.pyc\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    586\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    587\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchunked\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 588\u001b[0;31m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_read_chunked\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    589\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    590\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mamt\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\knotsupavit\\Anaconda2\\lib\\httplib.pyc\u001b[0m in \u001b[0;36m_read_chunked\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    628\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[0mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    629\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mchunk_left\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 630\u001b[0;31m                 \u001b[0mline\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_MAXLINE\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    631\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0m_MAXLINE\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    632\u001b[0m                     \u001b[1;32mraise\u001b[0m \u001b[0mLineTooLong\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"chunk size\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\knotsupavit\\Anaconda2\\lib\\socket.pyc\u001b[0m in \u001b[0;36mreadline\u001b[0;34m(self, size)\u001b[0m\n\u001b[1;32m    478\u001b[0m             \u001b[1;32mwhile\u001b[0m \u001b[0mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 480\u001b[0;31m                     \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_rbufsize\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    481\u001b[0m                 \u001b[1;32mexcept\u001b[0m \u001b[0merror\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    482\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mEINTR\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\knotsupavit\\Anaconda2\\lib\\ssl.pyc\u001b[0m in \u001b[0;36mrecv\u001b[0;34m(self, buflen, flags)\u001b[0m\n\u001b[1;32m    754\u001b[0m                     \u001b[1;34m\"non-zero flags not allowed in calls to recv() on %s\"\u001b[0m \u001b[1;33m%\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    755\u001b[0m                     self.__class__)\n\u001b[0;32m--> 756\u001b[0;31m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbuflen\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    757\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    758\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbuflen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\knotsupavit\\Anaconda2\\lib\\ssl.pyc\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m    641\u001b[0m                 \u001b[0mv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    642\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 643\u001b[0;31m                 \u001b[0mv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    644\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mv\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    645\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mSSLError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Import the necessary methods from tweepy library\n",
    "from tweepy.streaming import StreamListener\n",
    "from tweepy import OAuthHandler\n",
    "from tweepy import Stream\n",
    "import json\n",
    "\n",
    "#This is a basic listener that just prints received tweets to stdout.\n",
    "class MyListener(StreamListener):\n",
    "\n",
    "    def on_data(self, data):\n",
    "        try:\n",
    "            with open('tweets.json', 'a') as f:\n",
    "                f.write(data)\n",
    "                dat = json.loads(data)\n",
    "                print \"%s %s\" % (dat['created_at'], dat['text'])\n",
    "                return True\n",
    "        except BaseException as e:\n",
    "            print(\"--> Error on_data: %s\" % str(e))\n",
    "            pass\n",
    "        return True\n",
    "\n",
    "    def on_error(self, status):\n",
    "        print status\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    #This handles Twitter authetification and the connection to Twitter Streaming API\n",
    "    auth = OAuthHandler(consumer_key, consumer_secret)\n",
    "    auth.set_access_token(access_token, access_token_secret)\n",
    "    twitter_stream = Stream(auth, MyListener())\n",
    "\n",
    "    #This line filter Twitter Streams to capture data by the keywords: 'python', 'javascript', 'ruby'\n",
    "    twitter_stream.filter(track=['iphone', 'iphone7', 'iphone7plus', 'apple watch', 'ipad', 'samsung', 'ios', 'iOS9', 'microsoft', 'iOS10','android', 'galaxy note', 'oppo', 'acer', 'allview', 'alcatel', 'amazon', 'amoi', 'apple', 'archos', 'asus', 'at&t', 'benefon', 'benq', 'benq-siemens', 'blackberry', 'blu', 'bosch', 'bq', 'casio', 'celkon', 'chea', 'coolpad', 'dell', 'emporia', 'energizer', 'ericsson', 'eten', 'fujitsu', 'german-asus', 'gigabyte', 'gionee', 'haier', 'hp', 'htc', 'huawei', 'i-mate', 'i-mobile', 'icemobile', 'innostream', 'inq', 'intex', 'jolla', 'karbonn', 'kyocera', 'lava phone', 'leeco', 'lenovo', 'lg', 'maxon', 'maxwest', 'meizu', 'micromax', 'microsoft', 'modu', 'motorola', 'mwg', 'nec', 'neonode', 'niu', 'nokia', 'nvidia', 'o2', 'oneplus', 'oppo', 'orange', 'palm', 'panasonic', 'pantech', 'parla', 'philips', 'plum', 'posh', 'prestigio', 'qmobile', 'qtek', 'sagem', 'samsung', 'sendo', 'sewon', 'sharp', 'siemens', 'sonim', 'sony', 'sony ericsson', 'spice', 't-mobile', 'tel.me.', 'telit', 'toshiba', 'unnecto', 'vertu', 'verykool', 'vivo', 'vk mobile', 'vodafone', 'wiko', 'xiaomi', 'xolo', 'yezz', 'zte'], languages=['en'])\n",
    "    #twitter_stream.filter(track=['*'], languages=['th'])\n",
    "    #twitter_stream.filter(track=['*'])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading and Understanding the data\n",
    "The data that we stored in tweets.json is in **JSON** format. JSON stands for *JavaScript Object Notation*. This format makes it easy to humans to read the data, and for machines to parse it. Below is an example for one tweet in JSON format. You can see that the tweet contains additional information in addition to the main text which in this example: \"*How #BigData and CRM are Shaping Modern Marketing https:\\/\\/t.co\\/TgUYSUp9jT https:\\/\\/t.co\\/V54kea8cT2*\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{\"created_at\":\"Wed Oct 26 16:32:49 +0000 2016\",\"id\":791316663312457728,\"id_str\":\"791316663312457728\",\"text\":\"How #BigData and CRM are Shaping Modern Marketing https:\\/\\/t.co\\/TgUYSUp9jT https:\\/\\/t.co\\/V54kea8cT2\",\"display_text_range\":[0,73],\"source\":\"\\u003ca href=\\\"http:\\/\\/www.sociallymap.com\\\" rel=\\\"nofollow\\\"\\u003eSociallymap\\u003c\\/a\\u003e\",\"truncated\":false,\"in_reply_to_status_id\":null,\"in_reply_to_status_id_str\":null,\"in_reply_to_user_id\":null,\"in_reply_to_user_id_str\":null,\"in_reply_to_screen_name\":null,\"user\":{\"id\":4327758735,\"id_str\":\"4327758735\",\"name\":\"Globe Trotter BI\",\"screen_name\":\"GlobeTrotter_BI\",\"location\":null,\"url\":null,\"description\":\"* R\\u00e9seau international de consultants BI *      #Data #BusinessIntelligence #bigdata #datascientist #datamanagement\",\"protected\":false,\"verified\":false,\"followers_count\":104,\"friends_count\":212,\"listed_count\":50,\"favourites_count\":13,\"statuses_count\":318,\"created_at\":\"Mon Nov 30 10:15:23 +0000 2015\",\"utc_offset\":null,\"time_zone\":null,\"geo_enabled\":false,\"lang\":\"fr\",\"contributors_enabled\":false,\"is_translator\":false,\"profile_background_color\":\"C0DEED\",\"profile_background_image_url\":\"http:\\/\\/abs.twimg.com\\/images\\/themes\\/theme1\\/bg.png\",\"profile_background_image_url_https\":....,\"favorited\":false,\"retweeted\":false,\"possibly_sensitive\":false,\"filter_level\":\"low\",\"lang\":\"en\",\"timestamp_ms\":\"1477499569143\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the remaining of this lab, we will be using 4 Python libraries; *json* for parsing the data, *pandas* for data manipulation, *matplotlib* for creating charts, and *re* for regular expressions. \n",
    "\n",
    "The *json* and *re* libraries are installed by default in Python. You should install *pandas* and *matplotlib* if you don't have them in your machine.\n",
    "\n",
    "We will start first by uploading *json* and *pandas* using the commands below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will read the data in into an array that we call tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "......................................................................\n",
      "......................................................................\n",
      "......................................................................\n",
      "......................................................................\n",
      "......................................................................\n",
      "......................................................................\n",
      "......................................................................\n",
      "......................................................................\n",
      "......................................................................\n",
      "......................................................................\n",
      "......................................................................\n",
      "......................................................................\n",
      "......................................................................\n",
      "................\n",
      "92633 tweets read.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "tweets_data_path = 'C:\\\\Users\\\\knotsupavit\\\\Desktop\\\\jupyter\\\\tweets_bigData_dataAnalytic.json'\n",
    "\n",
    "tweets_data = []\n",
    "tweets_file = open(tweets_data_path, \"r\")\n",
    "count = 0\n",
    "for line in tweets_file:\n",
    "    try:\n",
    "        count = count + 1\n",
    "        tweet = json.loads(line)\n",
    "        tweets_data.append(tweet)\n",
    "        if count%100 == 0:\n",
    "            sys.stdout.write('.')\n",
    "        if count%7000 == 0:\n",
    "            sys.stdout.write('\\n')\n",
    "    except Exception as e:\n",
    "        print e\n",
    "        continue\n",
    "print \"\\n%s tweets read.\" % (count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will structure the tweets data into a pandas *DataFrame* to simplify the data manipulation. We will start by creating an empty DataFrame called **tweets** using the following command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tweets = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will add 3 columns to the **tweets** DataFrame called *text*, *lang*, and *country*, in which *text* column  contains the tweet, *lang* column contains the language in which the tweet was written, and *country* the country from which the tweet was sent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tweets['text'] = map(lambda tweet: tweet.get('text', None), tweets_data)\n",
    "tweets['lang'] = map(lambda tweet: tweet.get('lang', None), tweets_data)\n",
    "tweets['country'] = map(lambda tweet: tweet['place']['country'] if tweet.get('place') != None else None, tweets_data)\n",
    "print tweets.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will create a chart describing the Top 15 countries from which the tweets were sent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "tweets_by_country = tweets['country'].value_counts()\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.tick_params(axis='x', labelsize=10)\n",
    "ax.tick_params(axis='y', labelsize=10)\n",
    "ax.set_xlabel('Countries', fontsize=12)\n",
    "ax.set_ylabel('Number of tweets' , fontsize=12)\n",
    "ax.set_title('Top 15 countries', fontsize=12, fontweight='bold')\n",
    "tweets_by_country[:15].plot(ax=ax, kind='bar', color='blue')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **YOUR TURN**\n",
    "#### Problem01:\n",
    "Create a chart describing the Top 15 languages in which the tweets were written."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "YOUR TURN NOW: \n",
    "1) create a chart describing the top 10 native languages of which \n",
    "the twitter users speak.\n",
    "2) create a chart describing ...\n",
    "''' \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
